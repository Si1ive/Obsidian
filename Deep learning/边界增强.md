# 边界增强
## 思路
1. 常规的算子是函数方法，直接调用的那种，但是也可以转换成网络模型，转换成模型可以替代AER文中使用sigmoid来提取的边缘特征信息？这样是不是更精细？
2. 对AB数据进行边缘信息提取，并且跟AB合并作为输入的一部分来增强训练
## 问题
1. 是否都像卢婷，边界信息是限制在大尺寸上提取的？
2. 如何提取的？
3. 提取完之后，是怎么跟原本的特征信息进行结合的，这个方法唯一吗？
4.  当有label或者说有变化图时，通过调包就可以获得边缘图像，如此容易提取的信息真的对图像的增强有帮助吗？
5. 卢婷论文中，LSTM进行特征更细节的特征提取后，得到的多个不同尺寸的差异特征信息是LSTM不同时刻得到的吗？怎么后面直接到解码了
6. 似乎是没看到dwconv,什么时候需要呢?
7. 卢婷中的上下采样是否太多了，不是会损失很多信息吗，怎么改善？
8. EGRCNN的encoder怎么没有初始化？
9. EGRCNN的损失计算方式很奇怪，跟之前都不一样，并且对于每一个dout都计算损失应该会使得损失函数变得很大吧？线性插值出的结果应该非常差啊?难道就是想通过这样的方式更精细的去更改参数？
10. 哪些地方是卷积一下就可以，哪些地方是卷积完归一又激活呢？
11. 是否是这些卷积的策略十分直接的影响了结果？边缘图像结果是不太好的，是不是应该改成两轮？
12. Vmamba中的可选项是什么东西？
13. 直接将AB按一个新建的维度拼接，放入mamba编码器报错，patch_embed尺寸不对
14. 是将两个数据融合卷积再放入STSS，还是两个数据分别卷积然后再STSS融合？
15. 每个STSS都要接受的是两个数据，但是对于编码层数据是来源于上一层，上一层一融合就没有两个数据了
16. STSS中为什么通道数都给弄成128，而EGR中通道数是逐渐变大的
17. 卢婷损失是如何计算的？
18. 现在的问题是，改完模型后，描边输出不出描边特征了,是因为轮数还不够，模型输出的答案还不够标准，描边也随着论述发生变化，所以描边部分的权重更新不了?
19. 均值损失有什么特性?
20. Mamba部分能不能加上预训练权重，如果Resnet做骨架，Mamba嵌在里面，这样可以加预训练权重吗？
21. 为什么要用编码，解码结构
22. 区分特征加减乘不同的目的
23. 为什么会有两张来源不同的图像？



## 问题解决
1. 又看了一篇武大的，甚至就在最后一层，即尺寸最大的一层提了一组边缘信息，所以大概率就是不用用很多层来提取边缘信息，找个最后的层一提就行了
2. 很简单，就普通的卷积，不过是结果单独作为输出计算损失，从而对边缘提取进行限制
3. 也很简单，没有唯一不唯一，单纯cat一下，卷积融合一下
4. A 这个问题在代码中体现不出来
5. 中间画的DAM层抽象了，其实与之前的编码结构是相同的
6. A
7. 并没有格外多，只有解码编码两轮上下采样，论文中的图画的不是直接看的
8. 因为这里的encoder和之前编写的方式不一样，里面出现的组件已经定义过了，这里相当于是一个forward
9. A
10. UP卷积归一激活；融合的UP两轮卷积归一激活；输出将通道改成2的卷积不归一；边缘＋d的融合一轮卷积归一激活；
11. A
12. A
13. A
14. 我选择后者，因为这样更能突出STSS的多尺度融合？
15. 每一层的结果是 卷积和LSTM共同结合产生的 不是由LSTM直接输出，所以两个数据卷积之后但没有经过LSTM的数据作为下一轮的两个数据是没有问题的，所以解决方案14是可行的
16. A
17. A
18. 是的，轮数多后,变化图像逐渐稳定，边缘的训练可以进行了，边缘就逐渐出来了
19. A
20. A
21. 这个结构的目的是为了解决输入输出长度任意的问题，将任意长度的输入转换成固定大小的中间状态，再解码输出成任意长度的数据
22. A
23. 平时做的都是同源图像，他用异源图像，提高门槛，同时异源图像本身就有更多的问题，所以对应的任务会变得更加饱满吧

# 论文
## 论文Lightweight 
### 简介
1. 无代码,但有模型图
2. 跨尺度融合
3. 双分支
4. 211 西南交通，硕士一作
### 特性
1. 轻量级
2. 双分支
3. 跨尺度耦合 DSAM
4. 边界约束 BCM
5. 细节特征提取 CSCM
### 双分支
1. 细节分支：提取双时态空间特征
![[Pasted image 20241020104041.png]]
细节分支模型图
![[Pasted image 20241020103535.png]]
SD到C_SD通过一个简单的卷积
1. 语义分支：提取双时态多层上下文语义
### DSAM
结合两个两个分支

## 论文EGRCNN
### 简介
1. 有代码，有模型图
2. 湖南大学985
### 模型流程
1. 多层特征提取，此处可以更换成Mamba，特征信息压缩的很小
2. 差异信息提取层，先将两特征信息结合初步提取差异信息
3. 引入LSTM对差异信息进一步提取
4. 解码层，与changeMamba相同，跳跃链接
5. 解码的最后两层额外提取边缘信息
6. 特殊在于损失的计算
### 边界约束
### 多层特征提取
### 差异信息提取
### 缝合思路
#### 引入ChangeMamba的最基础mamba编码层
1. 创建新的train
2. 克隆一份编码层为空的模型
3. changeMamba两个编码层，ab两数据在解码层再融合，而EGR是数据先融合，再编码。可选的方案有：
	1. 先融合数据，用一个vssm编码层
	2. 将数据按照两个vssm写，更改后续层？应该是最复杂的方案
	3. 将数据按照两个vssm写，然后每一层都引入一个融合模块，这个可以参照其他的mamba模型
	4. 按照EGR的编码曾写，把LSTM部分换成VSSBlock，这个目测是最简单的，因为发现VSSM短时间分析不明白
4. ChangeMamba的编码层只有四个输出，而EGR是有五个的，查查在哪不一样了
	1. 应该是EGR在一层没有池化
	2. 虽然没有池化了，但是又卷积并且也改变了通道数
	3. **确定ChangeMamba的下采样尺度和EGR后四个是相同的**没解决
5. 按照3.4来融合
	1. 首先input不用动了，因为进来先接触EGR的卷积，原本是啥就是啥
	2. EGR中编码层，先卷积，再LSTM，卷积改变通道，那么看一看Mamba中的STSS改没改变通道，没有
	3. STSS不改变任何大小，接收两个数据，然后以三种方式拼合
## 论文 AERnet
残差+注意力+三个分类器无偏固定卷积核边缘增强
![[Pasted image 20241030104752.png]]
### WFEN
The backbone of the WFEN is the architecture of the pretrained ResNet34 [40] before the global average pooling layer.
### GCFAM
![[Pasted image 20241030171124.png]]

应用缩放因子，以避免在训练过程中由于点积值过大而导致的梯度消失问题
### ERM
sigmoid和softmax的区分应用，将特征按照八个方向进行提取
![[Pasted image 20241031105206.png]]
### EGMT
异源
### 编码器
数据来源不同，尺寸不同，编码不共享双分支，因为如果共享的话需要将两张尺寸不同的数据进行线性插值到尺寸相同，但是一开始就插值肯定不行
## 论文MEGANet
## EGA模块
1. 输入为编码器低级特征，解码器高级特征，边缘检测器特征
2. 改到我的模型里，编码器低级特征变为差异特征，将AFF模块替换到EGA模块当中
3. 问题：
4. 
	1. 原本传入解码器的是差异融合特征，直接对A,B差异这三个特征进行一个简单的卷积融合就丢到VSSB当中了，不合理，如果改成EGA的话，丢到VSSB之前进行简单融合的特征只有两个，是不是会减少特征损失了 √
	2. 差异特征调为作为EGA的输入，去掉差异的融合，将差异逐个输入到EGA √
	3. 添加边缘提取模块，提出一个新的特征  √
	4. 高斯滤波器的卷积核权重不应该固定吗？sigma应该设置为可学习的参数吗？
		1. 参照武大那一篇边缘增强，他没有将卷积核固定，
		2. 因为sigma也不知道怎么设置，所以还是设置成可学习的比较好
	5. 卷积核为5，padding确定是2吗？
	6. AI生成的方式是，按照两个不同的标准差来生成两个卷积核，然后作差，MEGAN是用同一个卷积核卷积两次，然后原图像减去这个得到边缘，查阅资料，应该就是取两个卷积核  √
	7. 灰度图和编码器特征相乘
	8. 对原始图像提取拉普拉斯特征得到的是两个特征，融合一下？

5. 拉普拉斯理解：
	1. 对噪声敏感，所以使用前进行高斯去噪，高斯加拉普拉斯组合成LoG,
	通过高斯差DoG运算符实现，对应到代码中就是过滤器
	2. 拉普拉斯金字塔的细节，因为拉普拉斯特征要逐层降低尺寸会丢失信息，所以每一层都从最原始的拉普拉斯特征中进行提取
	3. 具体实现：
		1. 对图像先进行一个高斯滤波，编写固定的卷积核，卷积核根据输入channel数进行复制，卷积核的通道数即为卷积后图像的通道数，拉普拉斯要提取边缘灰度图特征，所以一通道就可以了
		2. 