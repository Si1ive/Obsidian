# BUG 
1. V4版本 预测值全变成了0  
	解决：清梯度，损失函数有问题，修改了损失函数，增加了清缓存
# ST
1. 103041 dims 64 batchsize 128 v5 改了lap的融合方式，没改LN和DDB卷积，AFF激活
	1.F1 81 到第24轮之后就不涨了
2. √ 103140 跟10341版本一样，lr提升了10.7倍，再跑一次
	1. F1 8968 推理F1 1024 9033得分全比ChangeMamba大 比在自己电脑上跑出的效果好
	2. 学习率在倒数第二轮才发生变化，所以把轮数拉高到300再跑
3. 103144 1485 v6 batchsize 12 epoch200  
	1. 时间变得特别长 跑了18个点
	2. 得分下降了，意思AFF的激活换拉了 45轮就停了 8857
4. 103145 1577 v6 batchsize 128 lr 0.00107 
	1. F1 8921 39轮就不涨了
5. 103165 1486 v5 300轮
6. 对103145进行增强， 修改lr 以及递减速率
7. 目前103140是最好的一版，所以修改学习率以及优化器要在这一版上修改,修改成bs128  lr1e-4×8  Adam修改成AdamW 引入5e-4
找出得分最高的一版，然后逐个试探，把可能优化的点都试一试
# Version
## Version1
1. 训练到49轮左右时lr迭代到了1e-5，得分提升了  
2. 又开了新一轮，直接把lr设定为1e-5，并且把迭代频率考察epoch数从15换成了10，得分再也提不动了  
3. 疑似10轮太少了，导致学习率拼命减，减过头更新不动了
## Version2
1. 将dim从64改成96开始  
	参数量直接从37 干到77  
	训练速度从1.5s左右干到15s左右  
	从不到十分钟干到仨小时  
1. dim改成80  
	参数量53  
	训练速度5s 一轮一小时 训练不动没训练
## Version3
1. 修改所有降通道操作，都改成先降四倍，再提两倍  
2. 移除AFF融合四个D的模块，将AFF  
3. 更改为可以融合边缘增强的架构，即解码器中每一层都输出一个结果，然后都计算到损失当中  
4. 解码器引入LSSM，边缘增强分支两个lap+一个差异融合作为分支，融合主干  
5. 修改了损失函数,使用Focal+lovasz
## Version5
1. 模型中的参数量分布不够合理
	1. 对于边缘特征信息，可以使用很少参数量的层来进行特征提取
		1. 但是对于第一个lap，我先直接把两张图片融合6通道直接压成1，毕竟还是两张图像这样一下丢失太多信息了，应该用两个lap对A,B两张图像分别特征提取，然后将两个边缘特征合起来，交给解码层
		2. 解码层，逐像素乘法，大小不一致时，必须有一个为1，因为通过expand把这个1重复，解决方案，使用repeat把lap复制到AB_VSS的大小
	2. 但是对于非浅层的特征不能使用很少参数量的层来处理，会使得特征丢失
2. 归一化使用的理由不够充分
	1. 卷积的偏置只有结合BN时才False
## Version6
修改了vmamba2中的DDB,VSSB2导致其他们版本的VSSM也都跟着变
1. 修改AFF中的tanh激活函数，tanh更适合序列任务，更替为sigmoid更适合图像的二分类任务
	1. 取值0-1，A设定为1-sigmoid，B设定为sigmoid
	2. V5的设计是主干做+1，分支做2-，修改后也保持这个思路，主干不操作，分支做1-，因为重要的还是主干，分支只是做补足
2. 修改编码层的差异提取模块，逐深度卷积改为普通卷积，然后激活从LN改为了BN，这一个改动参数量大很多
3. 调查了一下，没有把数据批数砍掉，所以不存在中间的数据批数很小，所以把除了SSM中的归一，剩下的全部改成BN试一试
4. 高斯卷积不需要偏置项

## Version7
1. 调整损失
	1. 增加解码层输出，即增加out层和损失函数
	2. 在最优的版本上加，如果加上结果更好则保留，反之剔除
	3. 观察一下参数量变化，增加out参数量变化不大，但是Flop增大很多
	4. 损失函数
		1. 二分类更适合二元交叉损失
			1. AERNet，在BCE的基础上增加了两个权重，这个权重由IOU得来，五个SWBCE堆叠得来